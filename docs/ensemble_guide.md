# üìñ Ensemble Module - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

## üéØ Ensemble Methods ‡∏ó‡∏µ‡πà‡∏°‡∏µ

### 1. **WeightedEnsemble** - Weighted Average
- ‡∏£‡∏ß‡∏° predictions ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
- ‡∏á‡πà‡∏≤‡∏¢ ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

### 2. **StackingEnsemble** - Stacking with Meta-Learner
- ‡πÉ‡∏ä‡πâ predictions ‡∏à‡∏≤‡∏Å base models ‡πÄ‡∏õ‡πá‡∏ô features
- Train meta-model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
- ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô

### 3. **create_voting_ensemble()** - Voting
- Majority vote (hard) ‡∏´‡∏£‡∏∑‡∏≠ average probabilities (soft)
- sklearn's VotingClassifier/VotingRegressor

### 4. **blend_predictions()** - Flexible Blending
- Average, Rank average, Geometric mean
- ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö predictions ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

### 5. **optimize_blend_weights()** - Optimal Weights
- ‡∏´‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- Grid search ‡∏´‡∏£‡∏∑‡∏≠ Random search

### 6. **DynamicEnsemble** - Dynamic Model Selection
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
- Advanced technique

---

## üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### Example 1: Weighted Ensemble (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)

```python
from kaggle_utils.ensemble import WeightedEnsemble
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge

# ‡∏™‡∏£‡πâ‡∏≤‡∏á models
rf = RandomForestRegressor(n_estimators=100, random_state=42)
gbm = GradientBoostingRegressor(n_estimators=100, random_state=42)
ridge = Ridge(alpha=1.0)

# Weighted Ensemble
ensemble = WeightedEnsemble(
    models=[rf, gbm, ridge],
    weights=[0.5, 0.3, 0.2],  # ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å RF ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    task='regression',
    verbose=True
)

# Train
ensemble.fit(X_train, y_train)

# Predict
predictions = ensemble.predict(X_test)
```

**Output:**
```
============================================================
üéØ Training Weighted Ensemble (3 models)
============================================================
Weights: ['0.500', '0.300', '0.200']
Training Models: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00]

‚úÖ All models trained successfully!
```

---

### Example 2: Stacking Ensemble (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!)

```python
from kaggle_utils.ensemble import StackingEnsemble
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.linear_model import Ridge, Lasso

# Base models (level 0)
base_models = [
    RandomForestRegressor(n_estimators=100, random_state=42),
    ExtraTreesRegressor(n_estimators=100, random_state=42),
    Lasso(alpha=0.1)
]

# Meta model (level 1)
meta_model = Ridge(alpha=1.0)

# Stacking
stacking = StackingEnsemble(
    base_models=base_models,
    meta_model=meta_model,
    cv=5,
    task='regression',
    verbose=True
)

# Train (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤!)
stacking.fit(X_train, y_train)

# Predict
predictions = stacking.predict(X_test)
```

**Output:**
```
============================================================
üéØ Training Stacking Ensemble
============================================================
Base Models: 3
Meta Model: Ridge
CV Folds: 5

üìä Step 1/3: Creating meta-features with 5-fold CV...
Training Base Models: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:30<00:00]
  Model 1 Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5
  Model 2 Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5
  Model 3 Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5

üìä Step 2/3: Training meta model...
‚úÖ Meta model trained!

üìä Step 3/3: Retraining base models on full data...
Retraining Base Models: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:45<00:00]

‚úÖ Stacking ensemble training complete!
```

---

### Example 3: Voting Ensemble

```python
from kaggle_utils.ensemble import create_voting_ensemble
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Models
models = [
    RandomForestClassifier(n_estimators=100, random_state=42),
    LogisticRegression(max_iter=1000, random_state=42),
    SVC(probability=True, random_state=42)
]

# Voting Ensemble
voting = create_voting_ensemble(
    models=models,
    voting='soft',  # 'soft' = average probabilities, 'hard' = majority vote
    weights=[0.5, 0.3, 0.2],
    task='classification',
    verbose=True
)

# Train
voting.fit(X_train, y_train)

# Predict
predictions = voting.predict(X_test)
```

**Output:**
```
‚úÖ Created Voting Classifier (soft voting) with 3 models
   Weights: [0.5, 0.3, 0.2]
```

---

### Example 4: Blend Predictions (‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö predictions ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)

```python
from kaggle_utils.ensemble import blend_predictions

# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ predictions ‡∏à‡∏≤‡∏Å 3 models ‡πÅ‡∏•‡πâ‡∏ß
pred1 = model1.predict(X_test)
pred2 = model2.predict(X_test)
pred3 = model3.predict(X_test)

# Method 1: Average (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
final_avg = blend_predictions(
    [pred1, pred2, pred3],
    weights=[0.5, 0.3, 0.2],
    method='average',
    verbose=True
)

# Method 2: Rank Average (‡∏î‡∏µ‡∏ñ‡πâ‡∏≤ predictions ‡∏°‡∏µ scale ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)
final_rank = blend_predictions(
    [pred1, pred2, pred3],
    weights=[0.5, 0.3, 0.2],
    method='rank',
    verbose=True
)

# Method 3: Geometric Mean (‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö positive predictions)
final_geo = blend_predictions(
    [pred1, pred2, pred3],
    weights=[0.5, 0.3, 0.2],
    method='geometric',
    verbose=True
)
```

**Output:**
```
============================================================
üîÄ Blending 3 predictions
============================================================
Method: average
Weights: ['0.500', '0.300', '0.200']
‚úÖ Blending complete!
   Shape: (10000,)
```

---

### Example 5: Optimize Blend Weights (‡∏´‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!)

```python
from kaggle_utils.ensemble import optimize_blend_weights, blend_predictions

# Step 1: Get predictions on validation set
pred1_val = model1.predict(X_val)
pred2_val = model2.predict(X_val)
pred3_val = model3.predict(X_val)

# Step 2: ‡∏´‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
optimal_weights = optimize_blend_weights(
    predictions_list=[pred1_val, pred2_val, pred3_val],
    y_true=y_val,
    metric='rmse',
    method='grid',  # ‡∏´‡∏£‡∏∑‡∏≠ 'random'
    n_trials=100,
    verbose=True
)

# Step 3: ‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏±‡∏ö test set
pred1_test = model1.predict(X_test)
pred2_test = model2.predict(X_test)
pred3_test = model3.predict(X_test)

final = blend_predictions(
    [pred1_test, pred2_test, pred3_test],
    weights=optimal_weights,
    method='average'
)
```

**Output:**
```
============================================================
üîç Optimizing blend weights (grid search)
============================================================
Models: 3
Metric: RMSE
Testing 1331 weight combinations...
Grid Search: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1331/1331 [00:15<00:00] Best Score: 1234.5678

‚úÖ Optimization complete!
   Best RMSE: 1234.5678
   Optimal Weights: ['0.600', '0.300', '0.100']
```

---

### Example 6: Dynamic Ensemble (Advanced)

```python
from kaggle_utils.ensemble import DynamicEnsemble
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeClassifier

# Base models
models = [
    RandomForestRegressor(n_estimators=100, random_state=42),
    GradientBoostingRegressor(n_estimators=100, random_state=42),
    Ridge(alpha=1.0)
]

# Selection model (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ model ‡πÑ‡∏´‡∏ô)
selection_model = DecisionTreeClassifier(max_depth=5, random_state=42)

# Dynamic Ensemble
dynamic = DynamicEnsemble(
    models=models,
    selection_model=selection_model,
    task='regression',
    verbose=True
)

# Train
dynamic.fit(X_train, y_train)

# Predict (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ô‡∏•‡∏∞ model!)
predictions = dynamic.predict(X_test)
```

---

## üéØ Complete Workflow Example

```python
from kaggle_utils.models import LGBWrapper, XGBWrapper, RandomForestWrapper
from kaggle_utils.ensemble import blend_predictions, optimize_blend_weights
from kaggle_utils.utils import create_submission
from sklearn.model_selection import train_test_split

# === 1. SPLIT DATA ===
X = train.drop(columns=['target'])
y = train['target']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# === 2. TRAIN MULTIPLE MODELS ===
print("üöÄ Training Models...")

# LightGBM
lgb = LGBWrapper(n_splits=5)
lgb.train(X_train, y_train, X_test=test)

# XGBoost
xgb = XGBWrapper(n_splits=5)
xgb.train(X_train, y_train, X_test=test)

# Random Forest
rf = RandomForestWrapper(n_splits=5)
rf.train(X_train, y_train, X_test=test)

# === 3. OPTIMIZE BLEND WEIGHTS ON VALIDATION SET ===
print("\nüîç Optimizing blend weights...")

# Get validation predictions
lgb_val = lgb.predict(X_val)
xgb_val = xgb.predict(X_val)
rf_val = rf.predict(X_val)

# Find optimal weights
optimal_weights = optimize_blend_weights(
    [lgb_val, xgb_val, rf_val],
    y_val,
    metric='rmse',
    method='grid'
)

# === 4. BLEND TEST PREDICTIONS ===
print("\nüîÄ Blending final predictions...")
final_pred = blend_predictions(
    [lgb.test_predictions, xgb.test_predictions, rf.test_predictions],
    weights=optimal_weights,
    method='average'
)

# === 5. CREATE SUBMISSION ===
create_submission(test['id'], final_pred, 'submission.csv')
```

---

## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Ensemble Methods

| Method | Complexity | Training Time | Accuracy | When to Use |
|--------|-----------|---------------|----------|-------------|
| **Weighted Average** | ‚≠ê | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Quick baseline, simple |
| **Voting** | ‚≠ê‚≠ê | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Classification, sklearn integration |
| **Blending** | ‚≠ê | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | When you have predictions already |
| **Stacking** | ‚≠ê‚≠ê‚≠ê | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Maximum accuracy, have time |
| **Dynamic** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Advanced, different models excel differently |

---

## üö® Tips & Best Practices

### 1. üìä **Diverse Models Work Best**
```python
# ‚ùå BAD - Similar models
models = [
    RandomForestRegressor(n_estimators=100),
    RandomForestRegressor(n_estimators=200),
    RandomForestRegressor(n_estimators=300)
]

# ‚úÖ GOOD - Diverse models
models = [
    RandomForestRegressor(),      # Tree-based
    Ridge(),                       # Linear
    GradientBoostingRegressor()   # Boosting
]
```

### 2. ‚öñÔ∏è **Start with Equal Weights**
```python
# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
ensemble = WeightedEnsemble(models, weights=None)  # Auto: [0.33, 0.33, 0.34]

# ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ optimize
optimal_weights = optimize_blend_weights(predictions, y_val)
```

### 3. üéØ **Use Validation Set for Weight Optimization**
```python
# ‚ùå WRONG - Optimize on training set (overfitting!)
optimal_weights = optimize_blend_weights(train_predictions, y_train)

# ‚úÖ CORRECT - Optimize on validation set
optimal_weights = optimize_blend_weights(val_predictions, y_val)

# Use for test
final = blend_predictions(test_predictions, weights=optimal_weights)
```

### 4. üîÑ **Stacking Tips**
```python
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stacking:
# - Base models ‡∏Ñ‡∏ß‡∏£‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
# - Meta model ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ (Ridge, Lasso)
# - ‡πÉ‡∏ä‡πâ CV ‚â• 5 folds

stacking = StackingEnsemble(
    base_models=[rf, gbm, lasso],  # ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
    meta_model=Ridge(alpha=1.0),   # ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢
    cv=5                           # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5
)
```

### 5. üìà **Diminishing Returns**
```python
# 2-3 models ‡∏°‡∏±‡∏Å‡∏î‡∏µ‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Å‡∏¥‡∏ô 5 models ‡∏°‡∏±‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

# Good
ensemble = WeightedEnsemble([lgb, xgb, rf])

# Overkill (‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°)
ensemble = WeightedEnsemble([lgb, xgb, rf, cat, gbm, et, ada, svm])
```

---

## üé® Advanced Techniques

### Multi-Level Stacking
```python
# Level 0: Base models
rf = RandomForestRegressor()
gbm = GradientBoostingRegressor()
lasso = Lasso()

# Level 1: Meta model (also an ensemble!)
meta_models = [Ridge(), ElasticNet()]
meta_ensemble = WeightedEnsemble(meta_models, weights=[0.6, 0.4])

# Stacking
stacking = StackingEnsemble(
    base_models=[rf, gbm, lasso],
    meta_model=meta_ensemble,  # Ensemble as meta!
    cv=5
)
```

### Cascading Ensemble
```python
# First layer
lgb = LGBWrapper(n_splits=5)
lgb.train(X_train, y_train)

# Second layer (use OOF predictions as features)
X_train_meta = np.column_stack([X_train, lgb.oof_predictions.reshape(-1, 1)])

xgb = XGBWrapper(n_splits=5)
xgb.train(X_train_meta, y_train)
```

---

## üìå Quick Reference

| Task | Function | Code |
|------|----------|------|
| Simple averaging | `WeightedEnsemble` | `WeightedEnsemble(models, weights=[0.5, 0.5])` |
| Best accuracy | `StackingEnsemble` | `StackingEnsemble(base_models, meta_model, cv=5)` |
| Classification vote | `create_voting_ensemble` | `create_voting_ensemble(models, voting='soft')` |
| Blend predictions | `blend_predictions` | `blend_predictions([pred1, pred2], weights=[0.6, 0.4])` |
| Find best weights | `optimize_blend_weights` | `optimize_blend_weights([pred1, pred2], y_val)` |

---

**üí° Pro Tip:** ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å simple ensemble (Weighted/Blending) ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ñ‡πà‡∏≠‡∏¢‡πÉ‡∏ä‡πâ Stacking!
