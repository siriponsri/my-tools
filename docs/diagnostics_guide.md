# üè• Diagnostics Module - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà ML ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏õ‡∏±‡∏ç‡∏´‡∏≤

---

## üìö ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ

### 1. üîç **Data Quality Check**
```python
from kaggle_utils.diagnostics import check_data_quality

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
report = check_data_quality(train_df, target_col='price', show_details=True)
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
- ‚úÖ Missing values (features ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50%)
- ‚úÖ Constant features (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô)
- ‚úÖ Duplicate features (correlation > 0.95)
- ‚úÖ High cardinality categorical features
- ‚úÖ Target distribution ‡πÅ‡∏•‡∏∞ class imbalance

**Output:**
```
üîç DATA QUALITY CHECK
=================================================================
‚ö†Ô∏è  3 features ‡∏°‡∏µ missing values > 50%
‚ö†Ô∏è  2 features ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô)

üí° Suggestions:
   1. ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏•‡∏ö features: ['col_A', 'col_B', 'col_C']
   2. ‡∏Ñ‡∏ß‡∏£‡∏•‡∏ö constant features: ['col_X', 'col_Y']
```

---

### 2. üö® **Data Leakage Detection**
```python
from kaggle_utils.diagnostics import detect_leakage

# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö data leakage
suspicious = detect_leakage(train_df, target_col='price', test_df=test_df, threshold=0.95)
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
- ‚úÖ Features ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏±‡∏ö target (>0.95)
- ‚úÖ Mutual Information ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
- ‚úÖ Train-Test distribution differences
- ‚úÖ Temporal leakage (‡∏à‡∏≤‡∏Å date columns)

**Output:**
```
‚ö†Ô∏è  ‡∏û‡∏ö 2 features ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏±‡∏ö target (>0.95):
   - feature_A: 0.9823
   - feature_B: 0.9756

‚ùå ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô Data Leakage! ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö features ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ
```

---

### 3. üîó **Multicollinearity Check**
```python
from kaggle_utils.diagnostics import check_multicollinearity

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö features ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ô‡∏™‡∏π‡∏á
high_corr = check_multicollinearity(X_train, threshold=0.8)
```

**Output:**
```
‚ö†Ô∏è  ‡∏û‡∏ö 15 feature pairs ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á (>0.8)

üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
   1. ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏•‡∏ö features ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
   2. ‡πÉ‡∏ä‡πâ PCA ‡∏´‡∏£‡∏∑‡∏≠ feature selection
   3. ‡πÉ‡∏ä‡πâ regularization (Ridge/Lasso)
```

---

### 4. ü§ñ **Model Recommendation**
```python
from kaggle_utils.diagnostics import suggest_models

# ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
recommendations = suggest_models(train_df, target_col='price', task='auto')
```

**‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏°:**
- üìä Dataset size (small/medium/large)
- üìà Sample/Feature ratio
- üìù Number of categorical features
- üéØ Task type (regression/classification)

**Output:**
```
üìä Dataset Info:
   - Task: REGRESSION
   - Samples: 5,000
   - Features: 50
   - Sample/Feature Ratio: 100.0

üéØ Recommended Models:
   1. ü•á LightGBM / XGBoost
   2. ü•à Random Forest
   3. ü•â CatBoost (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ categorical features ‡πÄ‡∏¢‡∏≠‡∏∞)
```

---

### 5. üìâ **Overfitting Detection**
```python
from kaggle_utils.diagnostics import detect_overfitting

# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö overfitting/underfitting
result = detect_overfitting(
    model=rf_model, 
    X_train=X_train, y_train=y_train,
    X_val=X_val, y_val=y_val,
    task='regression'
)
```

**Output:**
```
üìä Performance Metrics:
   Train RMSE: 1234.56
   Val RMSE: 2345.67
   Gap: 1111.11 (90.0%)

‚ùå OVERFITTING detected!
üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
   1. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training
   2. ‡πÉ‡∏ä‡πâ regularization (Ridge, Lasso)
   3. ‡∏•‡∏î model complexity (max_depth, n_estimators)
   4. ‡πÉ‡∏ä‡πâ early stopping
```

---

### 6. üìà **Learning Curve Analysis**
```python
from kaggle_utils.diagnostics import plot_learning_curve

# Plot learning curve
plot_learning_curve(model, X_train, y_train, cv=5)
```

**‡πÅ‡∏™‡∏î‡∏á:**
- üìä Training score vs Validation score
- üìà Score changes ‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î training set
- üîç ‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢ overfitting/underfitting ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

---

### 7. üè• **Quick Diagnosis (All-in-One)**
```python
from kaggle_utils.diagnostics import quick_diagnosis

# ‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏î‡πà‡∏ß‡∏ô - ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß!
report = quick_diagnosis(
    train_df=train_df,
    target_col='price',
    test_df=test_df,
    model=rf_model,  # optional
    X_val=X_val,     # optional
    y_val=y_val      # optional
)
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:**
1. ‚úÖ Data Quality
2. ‚úÖ Data Leakage
3. ‚úÖ Multicollinearity
4. ‚úÖ Model Recommendation
5. ‚úÖ Overfitting Check (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ model)

**Output:**
```
üè• QUICK DIAGNOSIS - Comprehensive Data & Model Check
=================================================================

[... ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ...]

üìã FINAL REPORT
=================================================================
‚ö†Ô∏è  ‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 5

üî¥ Priority Issues:
   1. ‡∏•‡∏ö constant features
   2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data leakage features
   3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ multicollinearity

üéØ Recommended Next Steps:
   1. ü•á LightGBM / XGBoost
   2. ‡πÉ‡∏ä‡πâ cross-validation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
   3. Plot learning curves ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö overfitting
```

---

### 8. üéØ **Feature Importance Analysis**
```python
from kaggle_utils.diagnostics import analyze_feature_importance_detailed

# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå feature importance ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
importance_df = analyze_feature_importance_detailed(
    model=trained_model,
    X=X_train,
    y=y_train,
    feature_names=X_train.columns,
    top_n=20
)
```

**‡πÅ‡∏™‡∏î‡∏á:**
- üìä Built-in feature importance
- üîÑ Permutation importance (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤)
- üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ features ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏•‡∏ö

---

## üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

### Example 1: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÉ‡∏´‡∏°‡πà
```python
from kaggle_utils.diagnostics import quick_diagnosis
from kaggle_utils.preprocessing import reduce_mem_usage

# 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# 2. ‡∏•‡∏î memory
train = reduce_mem_usage(train)
test = reduce_mem_usage(test)

# 3. ‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏î‡πà‡∏ß‡∏ô
report = quick_diagnosis(
    train_df=train,
    target_col='target',
    test_df=test
)

# 4. ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
# - ‡∏•‡∏ö constant features
# - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data leakage
# - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
```

### Example 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô Submit
```python
from kaggle_utils.diagnostics import (
    detect_leakage,
    detect_overfitting,
    plot_learning_curve
)
from sklearn.model_selection import train_test_split

# 1. Split data
X = train.drop(columns=['target'])
y = train['target']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data leakage
suspicious = detect_leakage(train, 'target', test)

# 3. Train model
model.fit(X_train, y_train)

# 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö overfitting
overfitting_report = detect_overfitting(model, X_train, y_train, X_val, y_val)

# 5. Plot learning curve
plot_learning_curve(model, X, y, cv=5)

# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‚Üí Submit!
# ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‚Üí ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
```

### Example 3: Feature Engineering Workflow
```python
from kaggle_utils.diagnostics import (
    check_multicollinearity,
    analyze_feature_importance_detailed
)

# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡πÉ‡∏´‡∏°‡πà
train['feature_new'] = train['a'] * train['b']

# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö multicollinearity
high_corr = check_multicollinearity(train.drop(columns=['target']))

# 3. Train model
model.fit(X_train, y_train)

# 4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå feature importance
importance_df = analyze_feature_importance_detailed(
    model, X_train, y_train, X_train.columns
)

# 5. ‡∏•‡∏ö features ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
features_to_keep = importance_df[importance_df['importance'] > 0.001]['feature'].tolist()
X_train_selected = X_train[features_to_keep]
```

---

## üéØ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÑ‡∏´‡∏ô?

| ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå | ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ |
|-----------|------------------|
| üÜï ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÉ‡∏´‡∏°‡πà | `quick_diagnosis()` |
| üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | `check_data_quality()` |
| üö® ‡∏™‡∏á‡∏™‡∏±‡∏¢‡∏ß‡πà‡∏≤‡∏°‡∏µ data leakage | `detect_leakage()` |
| ü§ñ ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏´‡∏ô | `suggest_models()` |
| üìä Train/Val score ‡∏´‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å | `detect_overfitting()` |
| üìà ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π learning curve | `plot_learning_curve()` |
| üéØ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features | `analyze_feature_importance_detailed()` |
| üîó Features ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô | `check_multicollinearity()` |

---

## ‚ö†Ô∏è ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

### Data Leakage ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
- Features ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (‡∏£‡∏π‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô)
- Features ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å target ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
- Test data ‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏´‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ training set

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
```python
# ‚ùå WRONG - Data Leakage!
train['price_mean'] = train.groupby('category')['price'].transform('mean')
# ‚Üê ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á future data

# ‚úÖ CORRECT
train['price_mean'] = train.groupby('category')['price'].transform(
    lambda x: x.expanding().mean().shift()
)
# ‚Üê ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
```

### Overfitting vs Underfitting

**Overfitting:**
- Train score ‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà Val score ‡πÅ‡∏¢‡πà
- Model ‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ pattern
- ‡πÅ‡∏Å‡πâ: Regularization, ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏•‡∏î complexity

**Underfitting:**
- ‡∏ó‡∏±‡πâ‡∏á Train ‡πÅ‡∏•‡∏∞ Val score ‡πÅ‡∏¢‡πà‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
- Model ‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏à‡∏±‡∏ö pattern ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
- ‡πÅ‡∏Å‡πâ: ‡πÄ‡∏û‡∏¥‡πà‡∏° complexity, ‡πÄ‡∏û‡∏¥‡πà‡∏° features

---

## üöÄ Integration ‡∏Å‡∏±‡∏ö Workflow

```python
from kaggle_utils import *
from kaggle_utils.diagnostics import quick_diagnosis

# 1. Setup
setup_colab()

# 2. Load & Inspect
train = pd.read_csv('train.csv')
quick_info(train)

# 3. Comprehensive Diagnosis
report = quick_diagnosis(train, target_col='target', test_df=test)

# 4. Clean Data (‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
train = train.drop(columns=report['quality_report']['constant_features'])

# 5. Train Models
rf = RandomForestWrapper(n_splits=5)
rf.train(X_train, y_train, X_test)

# 6. Check Overfitting
detect_overfitting(rf.models[0], X_train, y_train, X_val, y_val)

# 7. Submit
create_submission(test['id'], rf.test_predictions, 'submission.csv')
```

---

## üìñ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

‡∏î‡∏π examples ‡πÉ‡∏ô `examples/` folder:
- `example_diagnostics.ipynb` - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
- `example_beginner_workflow.ipynb` - Workflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà

---

**üí° Pro Tip:** ‡πÉ‡∏ä‡πâ `quick_diagnosis()` ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÉ‡∏´‡∏°‡πà!
