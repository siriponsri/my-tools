# üìä Metrics Guide

‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Evaluation Metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Kaggle Competitions

## üìö Table of Contents

- [Overview](#overview)
- [Regression Metrics](#regression-metrics)
- [Classification Metrics](#classification-metrics)
- [Kaggle-Specific Metrics](#kaggle-specific-metrics)
- [Custom Metrics](#custom-metrics)
- [Examples](#examples)
- [Tips & Best Practices](#tips--best-practices)

---

## Overview

Module ‡∏ô‡∏µ‡πâ‡∏°‡∏µ metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ó‡∏±‡πâ‡∏á regression ‡πÅ‡∏•‡∏∞ classification ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢

### Quick Start

```python
from kaggle_utils import (
    calculate_regression_metrics,
    calculate_classification_metrics,
    rmse, mae, rmsle
)

# Regression
metrics = calculate_regression_metrics(y_true, y_pred)
print(metrics)

# Classification
metrics = calculate_classification_metrics(y_true, y_pred)
print(metrics)
```

---

## Regression Metrics

### 1. RMSE (Root Mean Squared Error)

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏õ‡πá‡∏ô metric ‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î penalize errors ‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ errors ‡πÄ‡∏•‡πá‡∏Å

```python
from kaggle_utils import rmse

score = rmse(y_true, y_pred)
print(f"RMSE: {score:.4f}")
```

**‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°**:
- ‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ (minimum = 0)
- ‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö target variable
- Sensitive ‡∏Å‡∏±‡∏ö outliers

### 2. MAE (Mean Absolute Error)

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ penalize outliers ‡∏°‡∏≤‡∏Å

```python
from kaggle_utils import mae

score = mae(y_true, y_pred)
print(f"MAE: {score:.4f}")
```

**‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°**:
- ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á absolute errors
- ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ RMSE ‡πÄ‡∏™‡∏°‡∏≠
- ‡πÑ‡∏°‡πà sensitive ‡∏Å‡∏±‡∏ö outliers ‡πÄ‡∏ó‡πà‡∏≤ RMSE

### 3. MAPE (Mean Absolute Percentage Error)

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô percentage

```python
from kaggle_utils import mape

score = mape(y_true, y_pred)
print(f"MAPE: {score:.2f}%")
```

**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤ y_true ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ 0
- Biased ‡∏ï‡πà‡∏≠ predictions ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á

### 4. RMSLE (Root Mean Squared Log Error)

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠ target ‡∏°‡∏µ range ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ penalize underestimation

```python
from kaggle_utils import rmsle

score = rmsle(y_true, y_pred)
print(f"RMSLE: {score:.4f}")
```

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:
- ‡πÑ‡∏°‡πà sensitive ‡∏Å‡∏±‡∏ö scale
- Penalize underestimation ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ overestimation
- ‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ô Kaggle (‡πÄ‡∏ä‡πà‡∏ô House Prices competition)

**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- y_true ‡πÅ‡∏•‡∏∞ y_pred ‡∏ï‡πâ‡∏≠‡∏á >= 0

### 5. R¬≤ Score (Coefficient of Determination)

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ variance ‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà

```python
from kaggle_utils import r2_score_custom

score = r2_score_custom(y_true, y_pred)
print(f"R¬≤: {score:.4f}")
```

**‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°**:
- 1.0 = perfect prediction
- 0.0 = ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ mean
- < 0 = ‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ mean

### Calculate All Regression Metrics

```python
from kaggle_utils import calculate_regression_metrics

metrics = calculate_regression_metrics(y_true, y_pred, verbose=True)

# Output:
# {
#     'rmse': 12.34,
#     'mae': 9.56,
#     'mape': 5.67,
#     'r2': 0.89,
#     'rmsle': 0.23  # ‡∏ñ‡πâ‡∏≤ y >= 0
# }
```

---

## Classification Metrics

### 1. Accuracy

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠ classes ‡∏°‡∏µ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô

```python
from sklearn.metrics import accuracy_score

acc = accuracy_score(y_true, y_pred)
print(f"Accuracy: {acc:.4f}")
```

**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**:
- ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö imbalanced data
- ‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ error ‡∏°‡∏≤‡∏à‡∏≤‡∏Å class ‡πÑ‡∏´‡∏ô

### 2. Precision, Recall, F1-Score

```python
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
```

**‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°**:
- **Precision**: ‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà predict ‡πÄ‡∏õ‡πá‡∏ô positive ‡∏°‡∏µ positive ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà
- **Recall**: ‡∏à‡∏≤‡∏Å positive ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÄ‡∏£‡∏≤ catch ‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà
- **F1**: Harmonic mean ‡∏Ç‡∏≠‡∏á precision ‡πÅ‡∏•‡∏∞ recall

**‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£**:
- Cost of False Positive ‡∏™‡∏π‡∏á ‚Üí ‡πÄ‡∏ô‡πâ‡∏ô Precision
- Cost of False Negative ‡∏™‡∏π‡∏á ‚Üí ‡πÄ‡∏ô‡πâ‡∏ô Recall
- ‡∏™‡∏°‡∏î‡∏∏‡∏• ‚Üí ‡πÉ‡∏ä‡πâ F1

### 3. ROC-AUC (Area Under ROC Curve)

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: Binary classification, ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å classes

```python
from sklearn.metrics import roc_auc_score

# ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ probability predictions
auc = roc_auc_score(y_true, y_pred_proba)
print(f"ROC-AUC: {auc:.4f}")
```

**‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°**:
- 1.0 = perfect classifier
- 0.5 = random guess
- < 0.5 = worse than random

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:
- ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à threshold
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö imbalanced data

### 4. Log Loss

**‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ penalize confident wrong predictions

```python
from sklearn.metrics import log_loss

loss = log_loss(y_true, y_pred_proba)
print(f"Log Loss: {loss:.4f}")
```

**‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°**:
- ‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ (minimum = 0)
- Penalize wrong predictions ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å

### 5. Confusion Matrix Metrics

```python
from kaggle_utils import confusion_matrix_metrics

metrics = confusion_matrix_metrics(y_true, y_pred)

print(f"Accuracy: {metrics['accuracy']:.4f}")
print(f"Precision: {metrics['precision']:.4f}")
print(f"Recall: {metrics['recall']:.4f}")
print(f"F1-Score: {metrics['f1']:.4f}")
print(f"Specificity: {metrics['specificity']:.4f}")
```

### Calculate All Classification Metrics

```python
from kaggle_utils import calculate_classification_metrics

# Binary classification
metrics = calculate_classification_metrics(
    y_true, 
    y_pred,
    y_pred_proba=y_pred_proba,  # optional
    verbose=True
)

# Output:
# {
#     'accuracy': 0.85,
#     'precision': 0.83,
#     'recall': 0.87,
#     'f1': 0.85,
#     'roc_auc': 0.91,  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ y_pred_proba
#     'log_loss': 0.32   # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ y_pred_proba
# }
```

---

## Kaggle-Specific Metrics

### Using `kaggle_metric()`

‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ metric ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Kaggle competition ‡∏Å‡∏≥‡∏´‡∏ô‡∏î

```python
from kaggle_utils import kaggle_metric

# RMSE
score = kaggle_metric(y_true, y_pred, metric='rmse')

# RMSLE
score = kaggle_metric(y_true, y_pred, metric='rmsle')

# MAE
score = kaggle_metric(y_true, y_pred, metric='mae')

# ROC-AUC
score = kaggle_metric(y_true, y_pred_proba, metric='roc_auc')

# Log Loss
score = kaggle_metric(y_true, y_pred_proba, metric='log_loss')
```

### Common Kaggle Metrics

| Competition Type | Metric | Function |
|-----------------|--------|----------|
| House Prices | RMSLE | `rmsle()` |
| Titanic | Accuracy | `accuracy_score()` |
| Credit Default | ROC-AUC | `roc_auc_score()` |
| Sales Forecasting | RMSE | `rmse()` |

---

## Custom Metrics

### ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Metric ‡πÄ‡∏≠‡∏á

```python
import numpy as np

def custom_metric(y_true, y_pred):
    """
    Custom metric ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    """
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Weighted RMSE
    weights = np.where(y_true > threshold, 2.0, 1.0)
    mse = np.average((y_true - y_pred) ** 2, weights=weights)
    return np.sqrt(mse)

score = custom_metric(y_true, y_pred)
```

### ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Cross-Validation

```python
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score

# ‡∏™‡∏£‡πâ‡∏≤‡∏á scorer
custom_scorer = make_scorer(
    custom_metric,
    greater_is_better=False  # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ
)

# ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô CV
scores = cross_val_score(
    model, X, y,
    cv=5,
    scoring=custom_scorer
)
```

---

## Examples

### Example 1: Regression Competition

```python
from kaggle_utils import (
    calculate_regression_metrics,
    rmse, rmsle
)
import numpy as np

# Train model
model.fit(X_train, y_train)

# Predict
y_pred_train = model.predict(X_train)
y_pred_val = model.predict(X_val)

# Evaluate
print("=== Training Set ===")
train_metrics = calculate_regression_metrics(y_train, y_pred_train)

print("\n=== Validation Set ===")
val_metrics = calculate_regression_metrics(y_val, y_pred_val)

# Check for overfitting
train_rmse = rmse(y_train, y_pred_train)
val_rmse = rmse(y_val, y_pred_val)

print(f"\nTrain RMSE: {train_rmse:.4f}")
print(f"Val RMSE: {val_rmse:.4f}")
print(f"Gap: {val_rmse - train_rmse:.4f}")

if val_rmse > train_rmse * 1.2:
    print("‚ö†Ô∏è  Possible overfitting!")
```

### Example 2: Classification Competition

```python
from kaggle_utils import (
    calculate_classification_metrics,
    optimal_threshold
)

# Train model
model.fit(X_train, y_train)

# Predict probabilities
y_pred_proba = model.predict_proba(X_val)[:, 1]

# Find optimal threshold
best_threshold = optimal_threshold(
    y_val, 
    y_pred_proba,
    metric='f1'  # optimize F1-score
)

print(f"Optimal threshold: {best_threshold:.4f}")

# Use optimal threshold
y_pred = (y_pred_proba >= best_threshold).astype(int)

# Evaluate
metrics = calculate_classification_metrics(
    y_val,
    y_pred,
    y_pred_proba=y_pred_proba
)

print(f"ROC-AUC: {metrics['roc_auc']:.4f}")
print(f"F1-Score: {metrics['f1']:.4f}")
```

### Example 3: Custom Competition Metric

```python
# ‡∏™‡∏°‡∏°‡∏ï‡∏¥ competition ‡πÉ‡∏ä‡πâ metric: Mean Log Quadratic Error
def mean_log_quadratic_error(y_true, y_pred):
    """Custom Kaggle metric"""
    return np.mean(np.log1p((y_true - y_pred) ** 2))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á scorer
from sklearn.metrics import make_scorer
mlqe_scorer = make_scorer(
    mean_log_quadratic_error,
    greater_is_better=False
)

# ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô model comparison
from kaggle_utils import quick_model_comparison

results = quick_model_comparison(
    X_train, y_train,
    cv=5,
    scoring=mlqe_scorer
)
```

---

## Tips & Best Practices

### 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Metric ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Problem

**Regression**:
- Target ‡∏°‡∏µ outliers ‚Üí ‡πÉ‡∏ä‡πâ MAE ‡πÅ‡∏ó‡∏ô RMSE
- Target ‡∏°‡∏µ range ‡∏Å‡∏ß‡πâ‡∏≤‡∏á ‚Üí ‡πÉ‡∏ä‡πâ RMSLE
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ penalize errors ‡πÉ‡∏´‡∏ç‡πà ‚Üí ‡πÉ‡∏ä‡πâ RMSE
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ % error ‚Üí ‡πÉ‡∏ä‡πâ MAPE (‡∏£‡∏∞‡∏ß‡∏±‡∏á divide by zero)

**Classification**:
- Balanced classes ‚Üí Accuracy, F1
- Imbalanced classes ‚Üí ROC-AUC, Precision-Recall
- Cost of errors ‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‚Üí Custom metric
- Need probabilities ‚Üí Log Loss

### 2. Monitor Multiple Metrics

```python
# ‡∏≠‡∏¢‡πà‡∏≤‡∏î‡∏π metric ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß!
metrics = {
    'rmse': rmse(y_true, y_pred),
    'mae': mae(y_true, y_pred),
    'r2': r2_score(y_true, y_pred),
}

# ‡∏î‡∏π trade-offs
print(pd.DataFrame([metrics]))
```

### 3. Check Train vs Validation Gap

```python
# Rule of thumb:
# - Gap < 10% ‚Üí OK
# - Gap 10-20% ‚Üí ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ overfitting
# - Gap > 20% ‚Üí overfitting ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô

gap_percent = (val_score - train_score) / train_score * 100
print(f"Gap: {gap_percent:.1f}%")
```

### 4. Use Stratified CV for Classification

```python
from sklearn.model_selection import StratifiedKFold

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ KFold ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, val_idx in skf.split(X, y):
    # Train ‡πÅ‡∏•‡∏∞ evaluate
    pass
```

### 5. Threshold Optimization

```python
from kaggle_utils import optimal_threshold

# ‡∏´‡∏≤ threshold ‡∏ó‡∏µ‡πà optimize F1
best_threshold = optimal_threshold(
    y_true,
    y_pred_proba,
    metric='f1'
)

# ‡∏´‡∏£‡∏∑‡∏≠ maximize precision ‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ recall >= 0.8
best_threshold = optimal_threshold(
    y_true,
    y_pred_proba,
    metric='precision',
    min_recall=0.8
)
```

### 6. Report Metrics ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û

```python
from kaggle_utils import classification_report_custom

# ‡πÅ‡∏™‡∏î‡∏á report ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
report = classification_report_custom(y_true, y_pred)
print(report)
```

---

## Common Mistakes

### 1. ‡πÉ‡∏ä‡πâ Accuracy ‡∏Å‡∏±‡∏ö Imbalanced Data

```python
# ‚ùå Wrong
# Dataset: 95% class 0, 5% class 1
# Model ‡∏ó‡∏µ‡πà predict ‡∏ó‡∏∏‡∏Å‡∏≠‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô 0 ‚Üí Accuracy = 95%!

# ‚úÖ Correct
# ‡πÉ‡∏ä‡πâ ROC-AUC, F1-Score, Precision-Recall ‡πÅ‡∏ó‡∏ô
```

### 2. ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Probabilities

```python
# ‚ùå Wrong
y_pred = model.predict(X)  # hard predictions
auc = roc_auc_score(y_true, y_pred)  # ERROR!

# ‚úÖ Correct
y_pred_proba = model.predict_proba(X)[:, 1]
auc = roc_auc_score(y_true, y_pred_proba)
```

### 3. RMSLE ‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏•‡∏ö

```python
# ‚ùå Wrong
y_pred = model.predict(X)  # ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏•‡∏ö
score = rmsle(y_true, y_pred)  # ERROR!

# ‚úÖ Correct
y_pred = np.maximum(0, y_pred)  # clip ‡∏Ñ‡πà‡∏≤‡∏•‡∏ö
score = rmsle(y_true, y_pred)
```

### 4. ‡πÑ‡∏°‡πà Check for Overfitting

```python
# ‚úÖ Always compare train vs validation
train_score = rmse(y_train, y_pred_train)
val_score = rmse(y_val, y_pred_val)

print(f"Train: {train_score:.4f}")
print(f"Val: {val_score:.4f}")
print(f"Gap: {val_score - train_score:.4f}")
```

---

## ‡∏™‡∏£‡∏∏‡∏õ Metrics ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó

### Regression

| Metric | Best Value | Use When | Sensitive to Outliers |
|--------|------------|----------|----------------------|
| RMSE | 0 | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ penalize errors ‡πÉ‡∏´‡∏ç‡πà | ‚úÖ Yes |
| MAE | 0 | ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ penalize outliers | ‚ùå No |
| MAPE | 0 | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ % error | ‚ùå No |
| RMSLE | 0 | Target ‡∏°‡∏µ range ‡∏Å‡∏ß‡πâ‡∏≤‡∏á | ‚ùå No |
| R¬≤ | 1 | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π explained variance | ‚úÖ Yes |

### Classification

| Metric | Best Value | Use When | Need Probabilities |
|--------|------------|----------|-------------------|
| Accuracy | 1 | Balanced classes | ‚ùå No |
| Precision | 1 | Cost of FP ‡∏™‡∏π‡∏á | ‚ùå No |
| Recall | 1 | Cost of FN ‡∏™‡∏π‡∏á | ‚ùå No |
| F1-Score | 1 | Balance precision/recall | ‚ùå No |
| ROC-AUC | 1 | Imbalanced data | ‚úÖ Yes |
| Log Loss | 0 | Need calibrated probabilities | ‚úÖ Yes |

---

## References

- [Scikit-learn Metrics Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Kaggle Evaluation Metrics](https://www.kaggle.com/c/about/evaluation)
- [Understanding AUC-ROC](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)
