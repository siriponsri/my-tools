# üõ†Ô∏è Utils Guide

‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Utility Functions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Kaggle Competitions

## Table of Contents

- [Overview](#overview)
- [Environment Setup](#environment-setup)
- [Data Loading & Saving](#data-loading--saving)
- [Kaggle Integration](#kaggle-integration)
- [Submission](#submission)
- [Timing & Profiling](#timing--profiling)
- [Memory Management](#memory-management)
- [Examples](#examples)

---

## Overview

Module ‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á Kaggle ‡πÄ‡∏ä‡πà‡∏ô:
- Setup environment (Colab, Kaggle)
- ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission
- ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞ memory usage

---

## Environment Setup

### 1. `setup_colab()`

‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Google Colab environment

```python
from kaggle_utils import setup_colab

# Setup Colab
setup_colab()

# ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:
# - Mount Google Drive
# - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á kaggle-utils (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
```

**Output:**
```
============================================================
üöÄ Setting up Google Colab
============================================================

üìÅ Mounting Google Drive...
‚úÖ Drive mounted at: /content/drive

‚úÖ kaggle-utils already installed

============================================================
‚úÖ Colab setup complete!
============================================================
```

### 2. `setup_kaggle()`

‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Kaggle API

```python
from kaggle_utils import setup_kaggle

# Auto detect kaggle.json
setup_kaggle()

# ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏∏ path ‡πÄ‡∏≠‡∏á
setup_kaggle(kaggle_json_path='/path/to/kaggle.json')
```

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**
- ‡∏™‡∏£‡πâ‡∏≤‡∏á `~/.kaggle/` directory
- Copy `kaggle.json` ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
- Set permissions (chmod 600)

**‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏≤ kaggle.json:**
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://www.kaggle.com/
2. ‡∏Ñ‡∏•‡∏¥‡∏Å Account ‚Üí Create New API Token
3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î `kaggle.json`

### 3. `check_environment()`

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà

```python
from kaggle_utils import check_environment

env = check_environment()
print(env)

# Output:
# {
#     'environment': 'colab',  # 'colab', 'kaggle', 'local'
#     'python_version': '3.10.12',
#     'platform': 'Linux',
#     'gpu_available': True,
#     'ram_total_gb': 12.7
# }
```

---

## Data Loading & Saving

### 1. `load_data()`

‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ format)

```python
from kaggle_utils import load_data

# CSV
df = load_data('train.csv')

# Parquet (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CSV)
df = load_data('train.parquet')

# Excel
df = load_data('data.xlsx', sheet_name='Sheet1')

# JSON
df = load_data('data.json')

# Auto detect encoding
df = load_data('train.csv', encoding='auto')

# Show info
df = load_data('train.csv', show_info=True)
```

**Parameters:**
- `filepath` (str): Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
- `encoding` (str): 'utf-8', 'latin1', 'auto'
- `show_info` (bool): ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á load
- `**kwargs`: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏¢‡∏±‡∏á pandas.read_*

**Supported Formats:**
- `.csv` ‚Üí `pd.read_csv()`
- `.parquet`, `.pq` ‚Üí `pd.read_parquet()`
- `.xlsx`, `.xls` ‚Üí `pd.read_excel()`
- `.json` ‚Üí `pd.read_json()`
- `.feather` ‚Üí `pd.read_feather()`
- `.pickle`, `.pkl` ‚Üí `pd.read_pickle()`

### 2. `save_data()`

‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (auto detect format ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)

```python
from kaggle_utils import save_data

# CSV
save_data(df, 'output.csv')

# Parquet (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ - ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡πá‡∏Å)
save_data(df, 'output.parquet')

# Excel
save_data(df, 'output.xlsx', sheet_name='Results')

# JSON
save_data(df, 'output.json')

# CSV ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ index
save_data(df, 'output.csv', index=False)
```

**Tips:**
- ‡πÉ‡∏ä‡πâ Parquet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CSV 10-100x)
- ‡πÉ‡∏ä‡πâ CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö submission

---

## Kaggle Integration

### 1. `download_kaggle_dataset()`

‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î dataset ‡∏à‡∏≤‡∏Å Kaggle

```python
from kaggle_utils import download_kaggle_dataset

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î dataset
download_kaggle_dataset(
    dataset_name='house-prices-advanced-regression-techniques',
    path='./data'
)

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î competition data
download_kaggle_dataset(
    competition='titanic',
    path='./data',
    competition=True
)
```

**Parameters:**
- `dataset_name` (str): ‡∏ä‡∏∑‡πà‡∏≠ dataset ‡∏´‡∏£‡∏∑‡∏≠ competition
- `path` (str): ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå
- `competition` (bool): ‡πÄ‡∏õ‡πá‡∏ô competition ‡∏´‡∏£‡∏∑‡∏≠ dataset
- `unzip` (bool): ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå zip ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
```python
# House Prices Competition
download_kaggle_dataset(
    competition='house-prices-advanced-regression-techniques',
    path='./data',
    competition=True,
    unzip=True
)

# ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå:
# ./data/train.csv
# ./data/test.csv
# ./data/sample_submission.csv
```

---

## Submission

### `create_submission()`

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Kaggle

```python
from kaggle_utils import create_submission

# ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
create_submission(
    ids=test_ids,
    predictions=predictions,
    filename='submission.csv'
)

# Custom column names
create_submission(
    ids=test['PassengerId'],
    predictions=predictions,
    filename='submission.csv',
    id_column='PassengerId',
    target_column='Survived'
)

# With info
create_submission(
    ids=test_ids,
    predictions=predictions,
    filename='submission.csv',
    show_sample=True  # ‡πÅ‡∏™‡∏î‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
)
```

**Output:**
```
‚úÖ Submission file created: submission.csv
üìä Shape: (418, 2)
üíæ Size: 6.2 KB

Sample:
   PassengerId  Survived
0          892         0
1          893         1
2          894         0
3          895         0
4          896         1
```

**Default Column Names:**
- ID column: `'id'`
- Target column: `'target'`

---

## Timing & Profiling

### 1. `timer()` Decorator

Decorator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤

```python
from kaggle_utils import timer

@timer
def train_model(X, y):
    model = RandomForestClassifier()
    model.fit(X, y)
    return model

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
model = train_model(X_train, y_train)

# Output:
# ‚è±Ô∏è  train_model took 5.23 seconds
```

### 2. `Timer` Class

Context manager ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤

```python
from kaggle_utils import Timer

# ‡πÅ‡∏ö‡∏ö context manager
with Timer("Loading data"):
    df = pd.read_csv('large_file.csv')

# Output:
# ‚è±Ô∏è  Loading data took 12.45 seconds

# ‡πÅ‡∏ö‡∏ö manual
timer = Timer()
timer.start()

# ... do something ...

elapsed = timer.stop()
print(f"Elapsed: {elapsed:.2f} seconds")
```

**Advanced Usage:**
```python
from kaggle_utils import Timer

# ‡∏ß‡∏±‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
timer = Timer("Full Pipeline")

timer.lap("Loading data")
df = load_data('train.csv')

timer.lap("Preprocessing")
df_processed = preprocess(df)

timer.lap("Training")
model.fit(X_train, y_train)

timer.lap("Predicting")
predictions = model.predict(X_test)

timer.stop()

# Output:
# ‚è±Ô∏è  Loading data: 2.3s
# ‚è±Ô∏è  Preprocessing: 5.1s
# ‚è±Ô∏è  Training: 45.2s
# ‚è±Ô∏è  Predicting: 1.2s
# ‚è±Ô∏è  Full Pipeline took 53.8 seconds
```

---

## Memory Management

### 1. `reduce_mem_usage()`

‡∏•‡∏î memory usage ‡∏Ç‡∏≠‡∏á DataFrame

```python
from kaggle_utils import reduce_mem_usage

# ‡∏Å‡πà‡∏≠‡∏ô
print(f"Before: {df.memory_usage().sum() / 1024**2:.2f} MB")

# ‡∏•‡∏î memory
df = reduce_mem_usage(df, verbose=True)

# ‡∏´‡∏•‡∏±‡∏á
print(f"After: {df.memory_usage().sum() / 1024**2:.2f} MB")

# Output:
# Memory usage decreased from 450.32 MB to 112.58 MB (75.0% reduction)
```

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**
- ‡πÅ‡∏õ‡∏•‡∏á int64 ‚Üí int8/int16/int32 (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ)
- ‡πÅ‡∏õ‡∏•‡∏á float64 ‚Üí float32 (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ)
- ‡∏•‡∏î memory ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏µ‡∏¢ information

### 2. `memory_usage()`

‡∏î‡∏π memory usage ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£

```python
from kaggle_utils import memory_usage

# Single variable
print(memory_usage(df))
# Output: "df: 123.45 MB"

# Multiple variables
print(memory_usage(df, X_train, y_train, model))
# Output:
# df: 123.45 MB
# X_train: 45.67 MB
# y_train: 1.23 MB
# model: 15.89 MB
# Total: 186.24 MB
```

### 3. `estimate_file_size()`

‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å

```python
from kaggle_utils import estimate_file_size

size_mb = estimate_file_size(df, format='csv')
print(f"Estimated CSV size: {size_mb:.2f} MB")

size_mb = estimate_file_size(df, format='parquet')
print(f"Estimated Parquet size: {size_mb:.2f} MB")
```

---

## Additional Utilities

### `set_seed()`

‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reproducibility

```python
from kaggle_utils import set_seed

# Set seed ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á
set_seed(42)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤:
# - random
# - numpy
# - torch (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
# - tensorflow (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
```

### `notify()`

‡∏™‡πà‡∏á notification ‡πÄ‡∏°‡∏∑‡πà‡∏≠ training ‡πÄ‡∏™‡∏£‡πá‡∏à (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)

```python
from kaggle_utils import notify

# Train model (‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å)
model.fit(X_train, y_train)

# ‡∏™‡πà‡∏á notification
notify("Model training complete! üéâ")

# ‡∏ö‡∏ô Colab ‡∏à‡∏∞‡πÑ‡∏î‡πâ browser notification
```

---

## Examples

### Example 1: Complete Workflow

```python
from kaggle_utils import *

# 1. Setup (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Colab)
setup_colab()
setup_kaggle()

# 2. Download data
download_kaggle_dataset(
    competition='house-prices-advanced-regression-techniques',
    path='./data',
    competition=True
)

# 3. Load data
train = load_data('./data/train.csv', show_info=True)
test = load_data('./data/test.csv')

# 4. Reduce memory
train = reduce_mem_usage(train)
test = reduce_mem_usage(test)

# 5. Train model
@timer
def train_model(X, y):
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)
    return model

model = train_model(X_train, y_train)

# 6. Predict
predictions = model.predict(X_test)

# 7. Create submission
create_submission(
    ids=test['Id'],
    predictions=predictions,
    filename='submission.csv',
    id_column='Id',
    target_column='SalePrice',
    show_sample=True
)

# 8. Notify (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Colab)
notify("Pipeline complete! üéâ")
```

### Example 2: Large Dataset Handling

```python
from kaggle_utils import *

# Check environment
env = check_environment()
print(f"RAM available: {env['ram_total_gb']:.1f} GB")

# Load ‡∏ó‡∏µ‡∏•‡∏∞ chunk (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å)
chunks = []
for chunk in pd.read_csv('huge_file.csv', chunksize=100000):
    # Reduce memory per chunk
    chunk = reduce_mem_usage(chunk, verbose=False)
    chunks.append(chunk)

df = pd.concat(chunks, ignore_index=True)
print(f"Final memory: {memory_usage(df)}")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Parquet (‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ CSV ‡∏°‡∏≤‡∏Å)
save_data(df, 'processed.parquet')
```

### Example 3: Benchmark Different Formats

```python
from kaggle_utils import Timer, save_data, load_data
import pandas as pd

# ‡∏™‡∏£‡πâ‡∏≤‡∏á test data
df = pd.DataFrame(np.random.randn(1000000, 50))

formats = ['csv', 'parquet', 'feather', 'pickle']
results = []

for fmt in formats:
    filename = f'test.{fmt}'
    
    # Test write speed
    with Timer(f"Write {fmt}") as t_write:
        save_data(df, filename)
    
    # Test read speed
    with Timer(f"Read {fmt}") as t_read:
        df_loaded = load_data(filename)
    
    # File size
    size_mb = os.path.getsize(filename) / 1024**2
    
    results.append({
        'Format': fmt,
        'Write (s)': t_write.elapsed,
        'Read (s)': t_read.elapsed,
        'Size (MB)': size_mb
    })

print(pd.DataFrame(results))
```

---

## Tips & Best Practices

### 1. ‡πÉ‡∏ä‡πâ Parquet ‡πÅ‡∏ó‡∏ô CSV

```python
# ‚ùå Slow
df = pd.read_csv('large_file.csv')  # 30 seconds
df.to_csv('output.csv')  # 45 seconds

# ‚úÖ Fast
df = pd.read_parquet('large_file.parquet')  # 2 seconds
df.to_parquet('output.parquet')  # 3 seconds
```

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ Parquet:**
- ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CSV 10-100x
- ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ CSV 3-10x
- ‡πÄ‡∏Å‡πá‡∏ö data types ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö compression

### 2. Reduce Memory ‡πÄ‡∏™‡∏°‡∏≠

```python
# ‡∏•‡∏î memory ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á load
train = load_data('train.csv')
train = reduce_mem_usage(train)

test = load_data('test.csv')
test = reduce_mem_usage(test)

# ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM 50-75%!
```

### 3. Set Seed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reproducibility

```python
from kaggle_utils import set_seed

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ seed ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏° notebook
set_seed(42)

# ‡∏ó‡∏∏‡∏Å random operation ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
```

### 4. ‡πÉ‡∏ä‡πâ Timer ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Optimize

```python
from kaggle_utils import Timer

# ‡∏´‡∏≤ bottleneck
with Timer("Feature engineering"):
    X = create_features(df)  # 45s

with Timer("Training"):
    model.fit(X, y)  # 120s

# ‚Üí ‡∏Ñ‡∏ß‡∏£ optimize training!
```

### 5. Check Memory ‡∏Å‡πà‡∏≠‡∏ô Load

```python
from kaggle_utils import check_environment, estimate_file_size

# Check available RAM
env = check_environment()
ram_available = env['ram_total_gb']

# Estimate file size
file_size_gb = os.path.getsize('huge.csv') / 1024**3

if file_size_gb > ram_available * 0.5:
    print("‚ö†Ô∏è  File too large! Use chunking:")
    # Load ‡∏ó‡∏µ‡∏•‡∏∞ chunk
    chunks = pd.read_csv('huge.csv', chunksize=100000)
else:
    df = load_data('huge.csv')
```

---

## Common Issues

### Issue 1: Kaggle API ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```python
# Error: 401 Unauthorized

# Fix:
# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ kaggle.json ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# 2. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î kaggle.json ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å kaggle.com
# 3. Run setup_kaggle() ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á

setup_kaggle(kaggle_json_path='/path/to/kaggle.json')
```

### Issue 2: Out of Memory

```python
# Error: MemoryError

# Fix 1: Reduce memory
df = reduce_mem_usage(df)

# Fix 2: Load ‡∏ó‡∏µ‡∏•‡∏∞ chunk
chunks = []
for chunk in pd.read_csv('file.csv', chunksize=50000):
    chunk = reduce_mem_usage(chunk, verbose=False)
    chunks.append(chunk)
df = pd.concat(chunks)

# Fix 3: ‡πÉ‡∏ä‡πâ fewer columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])
```

### Issue 3: Encoding Error

```python
# Error: UnicodeDecodeError

# Fix: ‡πÉ‡∏ä‡πâ encoding='auto'
df = load_data('file.csv', encoding='auto')

# ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ encodings
for enc in ['utf-8', 'latin1', 'cp1252']:
    try:
        df = pd.read_csv('file.csv', encoding=enc)
        print(f"‚úÖ Success with {enc}")
        break
    except:
        continue
```

---

## ‡∏™‡∏£‡∏∏‡∏õ Functions

### Environment
- `setup_colab()` - Setup Google Colab
- `setup_kaggle()` - Setup Kaggle API
- `check_environment()` - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment

### Data I/O
- `load_data()` - ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (CSV, Parquet, etc.)
- `save_data()` - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- `download_kaggle_dataset()` - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å Kaggle

### Submission
- `create_submission()` - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission

### Timing
- `timer()` - Decorator ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤
- `Timer` - Class ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤

### Memory
- `reduce_mem_usage()` - ‡∏•‡∏î memory usage
- `memory_usage()` - ‡∏î‡∏π memory usage
- `estimate_file_size()` - ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå

### Others
- `set_seed()` - Set random seed
- `notify()` - ‡∏™‡πà‡∏á notification
- `quick_info()` - ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ (from preprocessing)

---

## References

- [Pandas I/O Documentation](https://pandas.pydata.org/docs/user_guide/io.html)
- [Kaggle API Documentation](https://github.com/Kaggle/kaggle-api)
- [Parquet Format](https://parquet.apache.org/)
