# ‚öôÔ∏è Hyperparameter Tuning Guide

‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö hyperparameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning

## üìö Table of Contents
- [Overview](#overview)
- [Available Methods](#available-methods)
- [Grid Search](#grid-search)
- [Random Search](#random-search)
- [Bayesian Optimization (Optuna)](#bayesian-optimization-optuna)
- [Preset Search Spaces](#preset-search-spaces)
- [Examples](#examples)
- [Tips & Best Practices](#tips--best-practices)

---

## Overview

Module ‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏´‡∏≤ hyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 3 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å:
1. **Grid Search** - ‡∏•‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô grid
2. **Random Search** - ‡∏™‡∏∏‡πà‡∏°‡∏•‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
3. **Bayesian Optimization** - ‡πÉ‡∏ä‡πâ Optuna ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞

---

## Available Methods

### 1. `tune_hyperparameters()`
‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö hyperparameters

```python
from kaggle_utils import tune_hyperparameters

# ‡πÉ‡∏ä‡πâ Bayesian Optimization (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
best_params = tune_hyperparameters(
    model=LGBMRegressor(),
    param_space=param_space,
    X=X_train,
    y=y_train,
    method='bayesian',  # 'grid', 'random', 'bayesian'
    cv=5,
    n_trials=50,  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bayesian/random
    scoring='neg_mean_squared_error',
    verbose=True
)
```

### 2. `grid_search_cv()`
Grid Search with Cross-Validation

```python
from kaggle_utils import grid_search_cv

results = grid_search_cv(
    model=RandomForestRegressor(),
    param_grid={
        'n_estimators': [100, 200, 500],
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10]
    },
    X=X_train,
    y=y_train,
    cv=5,
    scoring='r2'
)
```

### 3. `random_search_cv()`
Random Search with Cross-Validation

```python
from kaggle_utils import random_search_cv

results = random_search_cv(
    model=XGBRegressor(),
    param_distributions=param_space,
    X=X_train,
    y=y_train,
    n_iter=50,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏∏‡πà‡∏°‡∏•‡∏≠‡∏á
    cv=5
)
```

### 4. `bayesian_optimization()`
Bayesian Optimization ‡∏î‡πâ‡∏ß‡∏¢ Optuna (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)

```python
from kaggle_utils import bayesian_optimization

best_params = bayesian_optimization(
    objective_func=objective,
    n_trials=100,
    direction='minimize',  # 'minimize' ‡∏´‡∏£‡∏∑‡∏≠ 'maximize'
    study_name='my_optimization',
    verbose=True
)
```

---

## Preset Search Spaces

‡∏°‡∏µ preset search spaces ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°:

### LightGBM

```python
from kaggle_utils import suggest_params_lightgbm

# ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á search space: 'narrow', 'default', 'wide'
params = suggest_params_lightgbm(
    search_space='default',
    task='regression'  # 'regression' ‡∏´‡∏£‡∏∑‡∏≠ 'classification'
)

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á output:
# {
#     'learning_rate': [0.01, 0.05, 0.1],
#     'n_estimators': [100, 500, 1000],
#     'num_leaves': [31, 50, 100],
#     'max_depth': [-1, 5, 10],
#     'min_child_samples': [20, 50, 100],
#     'subsample': [0.8, 0.9, 1.0],
#     'colsample_bytree': [0.8, 0.9, 1.0]
# }
```

### XGBoost

```python
from kaggle_utils import suggest_params_xgboost

params = suggest_params_xgboost(
    search_space='default',
    task='regression'
)

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á output:
# {
#     'learning_rate': [0.01, 0.05, 0.1],
#     'n_estimators': [100, 500, 1000],
#     'max_depth': [3, 5, 7],
#     'min_child_weight': [1, 3, 5],
#     'subsample': [0.8, 0.9, 1.0],
#     'colsample_bytree': [0.8, 0.9, 1.0]
# }
```

### CatBoost

```python
from kaggle_utils import suggest_params_catboost

params = suggest_params_catboost(
    search_space='default',
    task='classification'
)
```

### Random Forest

```python
from kaggle_utils import suggest_params_random_forest

params = suggest_params_random_forest(
    search_space='wide'
)

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á output:
# {
#     'n_estimators': [50, 100, 200, 500, 1000],
#     'max_depth': [None, 5, 10, 15, 20],
#     'min_samples_split': [2, 5, 10, 20],
#     'min_samples_leaf': [1, 2, 4, 8],
#     'max_features': ['auto', 'sqrt', 'log2']
# }
```

---

## Examples

### Example 1: Quick Tuning ‡∏î‡πâ‡∏ß‡∏¢ Grid Search

```python
from kaggle_utils import grid_search_cv, suggest_params_lightgbm
from lightgbm import LGBMRegressor

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° data
X_train, y_train = ...

# ‡πÉ‡∏ä‡πâ preset params
param_grid = suggest_params_lightgbm(search_space='narrow')

# Tune!
results = grid_search_cv(
    model=LGBMRegressor(),
    param_grid=param_grid,
    X=X_train,
    y=y_train,
    cv=5,
    verbose=True
)

print(f"Best params: {results['best_params']}")
print(f"Best score: {results['best_score']:.4f}")
```

### Example 2: Random Search ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost

```python
from kaggle_utils import random_search_cv, suggest_params_xgboost
from xgboost import XGBClassifier

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° data
X_train, y_train = ...

# ‡πÉ‡∏ä‡πâ preset params ‡πÅ‡∏ö‡∏ö wide
param_space = suggest_params_xgboost(
    search_space='wide',
    task='classification'
)

# Random search (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Grid Search)
results = random_search_cv(
    model=XGBClassifier(),
    param_distributions=param_space,
    X=X_train,
    y=y_train,
    n_iter=50,  # ‡∏•‡∏≠‡∏á 50 combinations
    cv=5,
    scoring='roc_auc'
)

# ‡πÉ‡∏ä‡πâ best params
best_model = XGBClassifier(**results['best_params'])
best_model.fit(X_train, y_train)
```

### Example 3: Bayesian Optimization (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)

```python
from kaggle_utils import bayesian_optimization
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score
import optuna

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° data
X_train, y_train = ...

# ‡∏™‡∏£‡πâ‡∏≤‡∏á objective function
def objective(trial):
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î search space
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
        'num_leaves': trial.suggest_int('num_leaves', 20, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
    }
    
    # Train ‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏î‡∏ú‡∏•
    model = LGBMRegressor(**params, random_state=42, verbose=-1)
    scores = cross_val_score(
        model, X_train, y_train,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )
    
    return -scores.mean()  # Optuna ‡∏à‡∏∞ minimize

# Run optimization
best_params = bayesian_optimization(
    objective_func=objective,
    n_trials=100,
    direction='minimize',
    study_name='lgbm_optimization',
    verbose=True
)

print(f"Best params: {best_params}")

# Train final model
final_model = LGBMRegressor(**best_params, random_state=42)
final_model.fit(X_train, y_train)
```

### Example 4: ‡πÉ‡∏ä‡πâ `tune_hyperparameters()` (‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!)

```python
from kaggle_utils import tune_hyperparameters, suggest_params_catboost
from catboost import CatBoostClassifier

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° data
X_train, y_train = ...

# ‡πÉ‡∏ä‡πâ preset params
param_space = suggest_params_catboost(
    search_space='default',
    task='classification'
)

# Tune ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢!
best_params = tune_hyperparameters(
    model=CatBoostClassifier(verbose=False),
    param_space=param_space,
    X=X_train,
    y=y_train,
    method='bayesian',  # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
    cv=5,
    n_trials=50,
    scoring='roc_auc',
    verbose=True
)

# Train final model
model = CatBoostClassifier(**best_params, verbose=False)
model.fit(X_train, y_train)
```

---

## Tips & Best Practices

### 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Method ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?

| Method | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ | ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡πÉ‡∏ä‡πâ |
|--------|-------|---------|--------------|
| **Grid Search** | ‡∏•‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤ ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô | ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å (exponential) | Search space ‡πÄ‡∏•‡πá‡∏Å, ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á |
| **Random Search** | ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Grid | ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ | Search space ‡πÉ‡∏´‡∏ç‡πà, ‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏≥‡∏Å‡∏±‡∏î |
| **Bayesian (Optuna)** | ‡∏â‡∏•‡∏≤‡∏î ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ | ‡∏ï‡πâ‡∏≠‡∏á install optuna | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥! ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ |

### 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Narrow ‚Üí Wide

```python
# ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å: ‡πÉ‡∏ä‡πâ narrow ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ
params_narrow = suggest_params_lightgbm('narrow')
results_narrow = tune_hyperparameters(..., param_space=params_narrow, n_trials=20)

# ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô: ‡∏Ç‡∏¢‡∏≤‡∏¢ search space ‡∏£‡∏≠‡∏ö‡πÜ best params
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç search space ‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ
```

### 3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å CV Folds ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

```python
# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢ (<10K): cv=5 ‡∏´‡∏£‡∏∑‡∏≠ cv=10
# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (10K-100K): cv=5
# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞ (>100K): cv=3 ‡∏´‡∏£‡∏∑‡∏≠ cv=5 (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
```

### 4. ‡πÉ‡∏ä‡πâ Scoring Metric ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

```python
# Regression
scoring='neg_mean_squared_error'  # RMSE
scoring='r2'                       # R¬≤
scoring='neg_mean_absolute_error'  # MAE

# Classification
scoring='roc_auc'        # ROC-AUC
scoring='accuracy'       # Accuracy
scoring='f1'            # F1-Score
scoring='precision'     # Precision
```

### 5. Monitor Overfitting

```python
# ‡∏î‡∏π gap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á train vs validation score
# ‡∏ñ‡πâ‡∏≤ gap ‡πÉ‡∏´‡∏ç‡πà ‚Üí overfitting

# ‡πÄ‡∏û‡∏¥‡πà‡∏° regularization:
# - L1: reg_alpha
# - L2: reg_lambda
# - min_child_samples, min_data_in_leaf
```

### 6. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Tune (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà)

```python
# Step 1: Fix learning_rate, tune structure params
params_step1 = {
    'learning_rate': 0.1,  # fix
    'n_estimators': [100, 500, 1000],
    'max_depth': [3, 5, 7, 10],
    'num_leaves': [31, 50, 100],
}

# Step 2: Tune sampling params
params_step2 = {
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
}

# Step 3: Tune regularization
params_step3 = {
    'reg_alpha': [0, 0.01, 0.1, 1.0],
    'reg_lambda': [0, 0.01, 0.1, 1.0],
}

# Step 4: Fine-tune learning_rate + n_estimators
params_step4 = {
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [500, 1000, 2000],
}
```

### 7. ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤

```python
# 1. ‡πÉ‡∏ä‡πâ n_jobs=-1 (parallel)
grid_search_cv(..., n_jobs=-1)

# 2. ‡πÉ‡∏ä‡πâ subset ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
X_sample = X_train.sample(n=10000, random_state=42)
y_sample = y_train.loc[X_sample.index]

# 3. ‡πÉ‡∏ä‡πâ early_stopping (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GBDT)
# ‡∏à‡∏∞‡∏´‡∏¢‡∏∏‡∏î train ‡πÄ‡∏£‡πá‡∏ß‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà improve
```

### 8. Save Results

```python
import json

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å best params
with open('best_params.json', 'w') as f:
    json.dump(results['best_params'], f, indent=2)

# ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ä‡πâ
with open('best_params.json', 'r') as f:
    best_params = json.load(f)

model = LGBMRegressor(**best_params)
```

---

## Common Issues

### Issue 1: Optuna ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
```python
# Error: optuna not available
# Fix:
pip install optuna
```

### Issue 2: ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å
```python
# ‡∏•‡∏î cv folds
cv=3  # ‡πÅ‡∏ó‡∏ô cv=5

# ‡∏•‡∏î n_trials
n_trials=20  # ‡πÅ‡∏ó‡∏ô n_trials=100

# ‡πÉ‡∏ä‡πâ subset ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
X_sample = X_train.sample(10000)
```

### Issue 3: Out of Memory
```python
# ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á
X_sample = X_train.sample(n=5000)

# ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î cv folds
cv=3
```

---

## ‡∏™‡∏£‡∏∏‡∏õ

| ‡∏£‡∏∞‡∏î‡∏±‡∏ö | ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ |
|-------|-----------|
| **‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà** | ‡πÉ‡∏ä‡πâ `suggest_params_*()` + Grid Search (`grid_search_cv`) |
| **‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á** | ‡πÉ‡∏ä‡πâ Random Search (`random_search_cv`) + preset narrow/default |
| **Advanced** | ‡πÉ‡∏ä‡πâ Bayesian Optimization (`bayesian_optimization`) + custom objective |

**Tip ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢:** ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ tune ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ! ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á feature engineering ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ üöÄ

---

## References

- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Scikit-learn Hyperparameter Tuning](https://scikit-learn.org/stable/modules/grid_search.html)
- [LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
- [XGBoost Parameters](https://xgboost.readthedocs.io/en/stable/parameter.html)
