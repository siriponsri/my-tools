# ğŸš€ Kaggle Utils

**à¸Šà¸¸à¸”à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸¡à¸·à¸­à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆà¸«à¸±à¸”à¹à¸‚à¹ˆà¸‡ Kaggle** ğŸ¯

Universal toolkit for Kaggle competitions - à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¸œà¸¹à¹‰à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹à¸¥à¸°à¸œà¸¹à¹‰à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸—à¸³ baseline à¹„à¸”à¹‰à¸£à¸§à¸”à¹€à¸£à¹‡à¸§

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- ğŸ¯ **à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆ** - à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹à¸‚à¹ˆà¸‡ Kaggle à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸‚à¸µà¸¢à¸™à¹‚à¸„à¹‰à¸”à¸‹à¹‰à¸³à¹†
- ğŸ” **Data Diagnostics** - à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸¸à¸“à¸ à¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸š leakage à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
- ğŸ¤– **Model Wrappers** - Train models à¸à¸£à¹‰à¸­à¸¡ CV à¹ƒà¸™à¸šà¸£à¸£à¸—à¸±à¸”à¹€à¸”à¸µà¸¢à¸§
- ğŸ¨ **Interactive Viz** - Visualizations à¹à¸šà¸š interactive à¸”à¹‰à¸§à¸¢ Plotly
- âš¡ **Fast & Easy** - à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¹€à¸§à¸¥à¸² focus à¸—à¸µà¹ˆ feature engineering à¹à¸¥à¸° modeling

## ğŸ“¦ Installation

### ğŸš€ Quick Start (à¸ªà¸³à¸«à¸£à¸±à¸š Colab/Kaggle) - à¹à¸™à¸°à¸™à¸³!

à¹ƒà¸Šà¹‰à¹€à¸à¸µà¸¢à¸‡ **2 à¸šà¸£à¸£à¸—à¸±à¸”** à¸šà¸™ Colab/Kaggle:

```python
!wget https://raw.githubusercontent.com/YOUR_USERNAME/kaggle-utils/main/kaggle_utils_single.py
from kaggle_utils_single import *
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- âš¡ à¸£à¸§à¸”à¹€à¸£à¹‡à¸§ (à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ install)
- ğŸ¯ à¸¡à¸µ functions à¸«à¸¥à¸±à¸à¹† à¸„à¸£à¸š
- ğŸ’¡ à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸š quick experiments

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­ Single-File Version â†’](docs/single_file_guide.md)**

---

### ğŸ“¦ Full Package Installation (à¸ªà¸³à¸«à¸£à¸±à¸š Local/Production)

```bash
# Basic installation (scikit-learn only)
pip install -e .

# Full installation (with LightGBM, XGBoost, CatBoost, Optuna)
pip install -e ".[full]"

# Or install from GitHub (à¹€à¸¡à¸·à¹ˆà¸­ upload à¹à¸¥à¹‰à¸§)
pip install git+https://github.com/yourusername/kaggle-utils.git
```

**à¸‚à¹‰à¸­à¸”à¸µ:**
- ğŸ¨ Features à¸„à¸£à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” (100+ functions)
- ğŸ¤– Model wrappers, Ensemble, Hyperparameter tuning
- ğŸ“Š Interactive visualizations
- ğŸ”§ Production-ready

## ğŸ¯ Quick Start

```python
from kaggle_utils import *

# 1. Setup environment (à¸ªà¸³à¸«à¸£à¸±à¸š Colab)
setup_colab()
setup_kaggle()

# 2. Load and inspect data
train = load_data('train.csv', show_info=True)
test = load_data('test.csv')

# 3. ğŸ” à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆ!)
report = quick_diagnosis(train, target_col='price', test_df=test)
# à¸ˆà¸°à¹à¸™à¸°à¸™à¸³à¸§à¹ˆà¸²à¸„à¸§à¸£à¸—à¸³à¸­à¸°à¹„à¸£à¸•à¹ˆà¸­!

# 4. Reduce memory
train = reduce_mem_usage(train)
test = reduce_mem_usage(test)

# 5. Quick model comparison
results = quick_model_comparison(X_train, y_train, cv=5)

# 6. Train best model with wrapper
lgb = LGBWrapper(n_splits=5, verbose=True)
lgb.train(X_train, y_train, X_test)

# 7. Create submission
create_submission(
    ids=test['Id'],
    predictions=lgb.test_predictions,
    filename='submission.csv',
    id_column='Id',
    target_column='SalePrice'
)
```

## ğŸ“š Modules Overview

### 1. ğŸ”§ Preprocessing (`preprocessing.py`)

**Data Inspection & Cleaning:**
- `quick_info()` - à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸£à¸¸à¸› missing values, dtypes
- `reduce_mem_usage()` - à¸¥à¸” memory usage 50-75%
- `handle_missing_values()` - à¸ˆà¸±à¸”à¸à¸²à¸£ missing values à¸«à¸¥à¸²à¸¢à¸§à¸´à¸˜à¸µ

**Feature Engineering:**
- `create_time_features()` - à¸ªà¸£à¹‰à¸²à¸‡ features à¸ˆà¸²à¸ datetime
- `create_polynomial_features()` - Polynomial features
- `create_interaction_features()` - Interaction features
- `create_aggregation_features()` - Group aggregation features
- `target_encode()` - Target encoding with smoothing
- `auto_feature_selection()` - à¹€à¸¥à¸·à¸­à¸ features à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/preprocessing_guide.md)**

---

### 2. ğŸ¤– Models (`models.py`)

**Scikit-Learn Wrappers (with built-in CV):**
- `SKLearnWrapper` - Universal wrapper à¸ªà¸³à¸«à¸£à¸±à¸š sklearn models
- `RandomForestWrapper` - Random Forest
- `RidgeWrapper`, `LassoWrapper`, `ElasticNetWrapper` - Linear models

**Gradient Boosting Wrappers:**
- `LGBWrapper` - LightGBM (fast & powerful!)
- `XGBWrapper` - XGBoost (stable & accurate)
- `CatBoostWrapper` - CatBoost (handles categorical well)

**Model Comparison:**
- `quick_model_comparison()` - à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸«à¸¥à¸²à¸¢ models à¸à¸£à¹‰à¸­à¸¡ CV
- `quick_classification_comparison()` - à¸ªà¸³à¸«à¸£à¸±à¸š classification
- `compare_scalers()` - à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š scalers

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/models_guide.md)**

---

### 3. ğŸ¯ Ensemble (`ensemble.py`)

**Ensemble Methods:**
- `WeightedEnsemble` - Weighted average ensemble
- `StackingEnsemble` - Stacking with meta-learner
- `DynamicEnsemble` - Dynamic weight optimization
- `create_voting_ensemble()` - Voting ensemble (hard/soft)

**Blending:**
- `blend_predictions()` - Average, rank, geometric mean
- `optimize_blend_weights()` - à¸«à¸²à¸™à¹‰à¸³à¸«à¸™à¸±à¸à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/ensemble_guide.md)**

---

### 4. ğŸ” Outliers (`outliers.py`)

**Detection Methods:**
- `detect_outliers_iqr()` - IQR method (classic)
- `detect_outliers_zscore()` - Z-score method
- `detect_outliers_isolation_forest()` - Isolation Forest
- `detect_outliers_lof()` - Local Outlier Factor
- `detect_outliers_ensemble()` - à¸£à¸§à¸¡à¸«à¸¥à¸²à¸¢à¸§à¸´à¸˜à¸µ

**Handling & Visualization:**
- `handle_outliers()` - Cap, remove, or transform
- `plot_outliers()` - Interactive outlier visualization
- `outlier_summary()` - à¸ªà¸£à¸¸à¸›à¸£à¸²à¸¢à¸‡à¸²à¸™ outliers

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/outliers_guide.md)**

---

### 5. âš™ï¸ Hyperparameters (`hyperparams.py`)

**Tuning Methods:**
- `tune_hyperparameters()` - All-in-one tuning function
- `grid_search_cv()` - Grid Search with CV
- `random_search_cv()` - Random Search with CV
- `bayesian_optimization()` - Bayesian Optimization (Optuna)

**Preset Search Spaces:**
- `suggest_params_lightgbm()` - LightGBM params (narrow/default/wide)
- `suggest_params_xgboost()` - XGBoost params
- `suggest_params_catboost()` - CatBoost params
- `suggest_params_random_forest()` - Random Forest params

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/hyperparams_guide.md)**

---

### 6. ğŸ¨ Visualization (`visualization.py`)

**Interactive Plots (Plotly):**
- `plot_feature_importance()` - Feature importance (single/comparison)
- `plot_distributions()` - Train vs test distributions
- `plot_correlation_heatmap()` - Correlation heatmap
- `plot_learning_curves()` - Learning curves
- `plot_confusion_matrix()` - Confusion matrix
- `plot_roc_curve()` - ROC curve
- `plot_predictions()` - Actual vs predicted

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/visualization_guide.md)**

---

### 7. ğŸ“Š Metrics (`metrics.py`)

**Regression Metrics:**
- `rmse()`, `mae()`, `mape()`, `rmsle()`, `r2_score_custom()`
- `calculate_regression_metrics()` - à¸„à¸³à¸™à¸§à¸“ metrics à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”

**Classification Metrics:**
- `calculate_classification_metrics()` - Accuracy, Precision, Recall, F1, ROC-AUC
- `confusion_matrix_metrics()` - Metrics from confusion matrix
- `optimal_threshold()` - à¸«à¸² threshold à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”

**Kaggle-Specific:**
- `kaggle_metric()` - à¹ƒà¸Šà¹‰ metric à¸•à¸²à¸¡à¸—à¸µà¹ˆ competition à¸à¸³à¸«à¸™à¸”

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/metrics_guide.md)**

---

### 8. ğŸ” Diagnostics (`diagnostics.py`) - **à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆ!** â­

**Data Quality:**
- `check_data_quality()` - à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸¸à¸“à¸ à¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¸£à¸§à¸¡
- `detect_leakage()` - à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸š data leakage
- `check_multicollinearity()` - à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š multicollinearity

**Model Diagnostics:**
- `suggest_models()` - à¹à¸™à¸°à¸™à¸³ models à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
- `detect_overfitting()` - à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸š overfitting/underfitting
- `plot_learning_curve()` - Learning curve analysis

**All-in-One:**
- `quick_diagnosis()` - ğŸŒŸ **à¸§à¸´à¸™à¸´à¸ˆà¸‰à¸±à¸¢à¸„à¸£à¸šà¸ˆà¸šà¹ƒà¸™à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹€à¸”à¸µà¸¢à¸§!**

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/diagnostics_guide.md)**

---

### 9. ğŸ› ï¸ Utils (`utils.py`)

**Environment Setup:**
- `setup_colab()` - Setup Google Colab
- `setup_kaggle()` - Setup Kaggle API
- `check_environment()` - à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š environment

**Data I/O:**
- `load_data()` - à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (CSV, Parquet, Excel, JSON)
- `save_data()` - à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (auto-detect format)
- `download_kaggle_dataset()` - à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¸ˆà¸²à¸ Kaggle

**Submission:**
- `create_submission()` - à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ submission

**Timing & Memory:**
- `timer()` - Decorator à¸§à¸±à¸”à¹€à¸§à¸¥à¸²
- `Timer` - Context manager à¸§à¸±à¸”à¹€à¸§à¸¥à¸²
- `memory_usage()` - à¸”à¸¹ memory usage

**Others:**
- `set_seed()` - Set random seed
- `notify()` - à¸ªà¹ˆà¸‡ notification (Colab)

ğŸ“– **[à¸­à¹ˆà¸²à¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹€à¸•à¹‡à¸¡ â†’](docs/utils_guide.md)**

---

## ğŸ’¡ Usage Examples

### Example 1: à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆ - Complete Pipeline

```python
from kaggle_utils import *
import pandas as pd

# 1. Setup (à¸–à¹‰à¸²à¹ƒà¸Šà¹‰ Colab)
setup_colab()
setup_kaggle()

# 2. Load data
train = load_data('train.csv', show_info=True)
test = load_data('test.csv')

# 3. ğŸ” à¸§à¸´à¸™à¸´à¸ˆà¸‰à¸±à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (à¸ªà¸³à¸„à¸±à¸!)
report = quick_diagnosis(train, target_col='SalePrice', test_df=test)
# à¸ˆà¸°à¸šà¸­à¸à¸§à¹ˆà¸²:
# - à¸¡à¸µ missing values à¹„à¸«à¸¡
# - à¸¡à¸µ data leakage à¹„à¸«à¸¡
# - à¸¡à¸µ outliers à¹„à¸«à¸¡
# - à¸„à¸§à¸£à¹ƒà¸Šà¹‰ model à¸­à¸°à¹„à¸£

# 4. Preprocess based on recommendations
train = reduce_mem_usage(train)
test = reduce_mem_usage(test)

# Handle missing values
train = handle_missing_values(train, strategy='auto')
test = handle_missing_values(test, strategy='auto')

# 5. Feature engineering
X_train = train.drop('SalePrice', axis=1)
y_train = train['SalePrice']
X_test = test.copy()

# 6. Quick model comparison
print("ğŸ” Comparing models...")
results = quick_model_comparison(X_train, y_train, cv=5)

# 7. Train best model
print("\nğŸš€ Training LightGBM...")
lgb = LGBWrapper(n_splits=5, verbose=True)
lgb.train(X_train, y_train, X_test)

# 8. Create submission
create_submission(
    ids=test['Id'],
    predictions=lgb.test_predictions,
    filename='submission.csv',
    id_column='Id',
    target_column='SalePrice'
)

print("\nâœ… Done! Ready to submit to Kaggle!")
```

### Example 2: Ensemble Multiple Models

```python
from kaggle_utils import *

# Train multiple models
models = {}

print("Training Random Forest...")
models['rf'] = RandomForestWrapper(n_estimators=100, n_splits=5)
models['rf'].train(X_train, y_train, X_test)

print("Training LightGBM...")
models['lgb'] = LGBWrapper(n_splits=5)
models['lgb'].train(X_train, y_train, X_test)

print("Training XGBoost...")
models['xgb'] = XGBWrapper(n_splits=5)
models['xgb'].train(X_train, y_train, X_test)

# Get predictions
predictions = [
    models['rf'].test_predictions,
    models['lgb'].test_predictions,
    models['xgb'].test_predictions
]

# Blend with optimal weights
final_pred = blend_predictions(
    predictions,
    method='optimize',  # à¸«à¸²à¸™à¹‰à¸³à¸«à¸™à¸±à¸à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
    y_true=y_val  # à¹ƒà¸Šà¹‰ validation set
)

# Create submission
create_submission(test['Id'], final_pred, 'ensemble_submission.csv')
```

### Example 3: Hyperparameter Tuning

```python
from kaggle_utils import *

# Get preset parameter space
params = suggest_params_lightgbm(
    search_space='default',
    task='regression'
)

# Tune with Bayesian Optimization (à¹à¸™à¸°à¸™à¸³!)
best_params = tune_hyperparameters(
    model=LGBMRegressor(),
    param_space=params,
    X=X_train,
    y=y_train,
    method='bayesian',
    cv=5,
    n_trials=50,
    verbose=True
)

print(f"Best params: {best_params}")

# Train final model with best params
final_model = LGBWrapper(**best_params, n_splits=5)
final_model.train(X_train, y_train, X_test)
```

### Example 4: Outlier Detection & Handling

```python
from kaggle_utils import *

# Detect outliers (à¸«à¸¥à¸²à¸¢à¸§à¸´à¸˜à¸µ)
outliers_iqr = detect_outliers_iqr(train, columns=['SalePrice', 'GrLivArea'])
outliers_iso = detect_outliers_isolation_forest(train)

# Visualize
plot_outliers(train, 'SalePrice', method='iqr')
plot_outliers_comparison(train, 'SalePrice')

# Handle outliers
train_clean = handle_outliers(
    train, 
    columns=['SalePrice', 'GrLivArea'],
    method='cap',  # 'cap', 'remove', 'winsorize'
    threshold=1.5
)

print(f"Removed {len(train) - len(train_clean)} outliers")
```

### Example 5: Feature Engineering Pipeline

```python
from kaggle_utils import *

# 1. Time features (à¸–à¹‰à¸²à¸¡à¸µ datetime)
train = create_time_features(train, date_col='transaction_date')

# 2. Polynomial features
X_poly = create_polynomial_features(
    train[numeric_cols],
    degree=2,
    include_bias=False
)

# 3. Interaction features
X_interact = create_interaction_features(
    train[['col1', 'col2', 'col3']],
    max_interactions=2
)

# 4. Aggregation features
train = create_aggregation_features(
    train,
    group_col='category',
    agg_cols=['price', 'quantity'],
    agg_funcs=['mean', 'std', 'max', 'min']
)

# 5. Target encoding
train, encoder = target_encode(
    train,
    categorical_cols=['category', 'brand'],
    target_col='price',
    smoothing=10
)

# 6. Auto feature selection
X_selected, selected_features = auto_feature_selection(
    X_train, y_train,
    task='regression',
    k=50  # select top 50 features
)

print(f"Selected features: {selected_features}")
```

---

## ğŸ“ For Beginners (à¸„à¸³à¹à¸™à¸°à¸™à¸³à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆ)

### à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£?

1. **à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸ Diagnostics à¹€à¸ªà¸¡à¸­!** ğŸ”

```python
# à¸—à¸¸à¸ competition à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸à¸™à¸µà¹‰
report = quick_diagnosis(train, target_col='target', test_df=test)
```

à¸£à¸°à¸šà¸šà¸ˆà¸°à¸šà¸­à¸à¸§à¹ˆà¸²:
- à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¡à¸µà¸›à¸±à¸à¸«à¸²à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡
- à¸„à¸§à¸£à¹ƒà¸Šà¹‰ model à¸­à¸°à¹„à¸£
- à¸¡à¸µ data leakage à¹„à¸«à¸¡
- à¸„à¸§à¸£à¸—à¸³à¸­à¸°à¹„à¸£à¸•à¹ˆà¸­

2. **à¹ƒà¸Šà¹‰ Model Comparison à¸«à¸² baseline** ğŸ¤–

```python
# à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸«à¸¥à¸²à¸¢ models à¹ƒà¸™à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§
results = quick_model_comparison(X_train, y_train, cv=5)
```

3. **Train à¸”à¹‰à¸§à¸¢ Wrappers (à¸¡à¸µ CV built-in)** âš¡

```python
# Train + CV à¹ƒà¸™à¸šà¸£à¸£à¸—à¸±à¸”à¹€à¸”à¸µà¸¢à¸§!
lgb = LGBWrapper(n_splits=5)
lgb.train(X_train, y_train, X_test)
```

4. **à¸ªà¸£à¹‰à¸²à¸‡ Submission** ğŸ“¤

```python
create_submission(test_ids, predictions, 'submission.csv')
```

### Workflow à¹à¸™à¸°à¸™à¸³

```python
# Step 1: Diagnose
report = quick_diagnosis(train, target_col='target', test_df=test)

# Step 2: Clean data
train = reduce_mem_usage(train)
train = handle_missing_values(train)

# Step 3: Compare models
results = quick_model_comparison(X_train, y_train)

# Step 4: Train best model
model = LGBWrapper(n_splits=5)
model.train(X_train, y_train, X_test)

# Step 5: Submit
create_submission(test_ids, model.test_predictions, 'submission.csv')
```

### Tips à¸ªà¸³à¸«à¸£à¸±à¸šà¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆ

âœ… **DO:**
- à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸ simple model à¸à¹ˆà¸­à¸™ (Random Forest, LightGBM)
- à¹ƒà¸Šà¹‰ `quick_diagnosis()` à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¹€à¸£à¸´à¹ˆà¸¡
- à¸¥à¸” memory à¸”à¹‰à¸§à¸¢ `reduce_mem_usage()`
- à¹ƒà¸Šà¹‰ CV à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 5 folds
- à¹€à¸Šà¹‡à¸„ train vs validation gap

âŒ **DON'T:**
- à¸­à¸¢à¹ˆà¸²à¹ƒà¸Šà¹‰ model à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¹à¸£à¸
- à¸­à¸¢à¹ˆà¸² optimize hyperparameters à¸à¹ˆà¸­à¸™à¸¡à¸µ good features
- à¸­à¸¢à¹ˆà¸²à¸¥à¸·à¸¡à¹€à¸Šà¹‡à¸„ data leakage
- à¸­à¸¢à¹ˆà¸²à¸—à¸³ feature engineering à¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸› à¹ƒà¸™à¸•à¸­à¸™à¹à¸£à¸

---

## ğŸ“ Project Structure

```
kaggle-utils/
â”œâ”€â”€ kaggle_utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py      # Data cleaning & feature engineering
â”‚   â”œâ”€â”€ models.py             # Model wrappers with CV
â”‚   â”œâ”€â”€ ensemble.py           # Ensemble methods
â”‚   â”œâ”€â”€ outliers.py           # Outlier detection & handling
â”‚   â”œâ”€â”€ hyperparams.py        # Hyperparameter tuning
â”‚   â”œâ”€â”€ visualization.py      # Interactive plots
â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics
â”‚   â”œâ”€â”€ diagnostics.py        # Data & model diagnostics
â”‚   â””â”€â”€ utils.py              # Utility functions
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ preprocessing_guide.md
â”‚   â”œâ”€â”€ models_guide.md
â”‚   â”œâ”€â”€ ensemble_guide.md
â”‚   â”œâ”€â”€ outliers_guide.md
â”‚   â”œâ”€â”€ hyperparams_guide.md
â”‚   â”œâ”€â”€ metrics_guide.md
â”‚   â”œâ”€â”€ visualization_guide.md
â”‚   â”œâ”€â”€ diagnostics_guide.md
â”‚   â””â”€â”€ utils_guide.md
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ”§ For Google Colab Users

```python
# à¸§à¸´à¸˜à¸µ 1: Install à¸ˆà¸²à¸ GitHub (à¹€à¸¡à¸·à¹ˆà¸­ upload à¹à¸¥à¹‰à¸§)
!pip install git+https://github.com/yourusername/kaggle-utils.git

# à¸§à¸´à¸˜à¸µ 2: Clone à¹à¸¥à¸° install
!git clone https://github.com/yourusername/kaggle-utils.git
%cd kaggle-utils
!pip install -e ".[full]"

# à¸§à¸´à¸˜à¸µ 3: Upload zip file
# 1. Upload kaggle-utils.zip to Colab
# 2. Unzip à¹à¸¥à¸° install
!unzip kaggle-utils.zip
%cd kaggle-utils
!pip install -e .

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
from kaggle_utils import *
setup_colab()  # Setup Colab environment
```

---

## ğŸ“– Documentation

à¹à¸•à¹ˆà¸¥à¸° module à¸¡à¸µà¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹à¸¢à¸à¹‚à¸”à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”:

- ğŸ“˜ [Preprocessing Guide](docs/preprocessing_guide.md) - Data cleaning & feature engineering
- ğŸ“™ [Models Guide](docs/models_guide.md) - Model training with CV
- ğŸ“• [Ensemble Guide](docs/ensemble_guide.md) - Ensemble methods
- ğŸ“— [Outliers Guide](docs/outliers_guide.md) - Outlier detection & handling
- ğŸ“” [Hyperparams Guide](docs/hyperparams_guide.md) - Hyperparameter tuning
- ğŸ““ [Metrics Guide](docs/metrics_guide.md) - Evaluation metrics
- ğŸ“– [Visualization Guide](docs/visualization_guide.md) - Interactive plots
- ğŸ“’ [Diagnostics Guide](docs/diagnostics_guide.md) - Data & model diagnostics â­
- ğŸ“‘ [Utils Guide](docs/utils_guide.md) - Utility functions

---

## ğŸš€ Features Highlights

### âš¡ Fast & Easy
- Train models with CV à¹ƒà¸™à¸šà¸£à¸£à¸—à¸±à¸”à¹€à¸”à¸µà¸¢à¸§
- Auto feature selection
- One-line submission creation

### ğŸ” Beginner-Friendly
- `quick_diagnosis()` - à¸§à¸´à¸™à¸´à¸ˆà¸‰à¸±à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸£à¸šà¸ˆà¸š
- Data leakage detection
- Model recommendations
- Clear error messages

### ğŸ¨ Interactive
- Plotly-based visualizations
- Progress bars à¸”à¹‰à¸§à¸¢ tqdm
- Real-time feedback

### ğŸ“Š Production-Ready
- Memory optimization (50-75% reduction)
- Type hints à¸ªà¸³à¸«à¸£à¸±à¸š IDE support
- Comprehensive documentation

---

## ğŸ¤ Contributing

Contributions are welcome! à¹€à¸£à¸²à¸¢à¸´à¸™à¸”à¸µà¸£à¸±à¸š:

- ğŸ› Bug reports
- ğŸ’¡ Feature requests
- ğŸ“ Documentation improvements
- ğŸ”§ Code contributions

### How to Contribute

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸¶à¹‰à¸™à¹€à¸à¸·à¹ˆà¸­à¸Šà¸¸à¸¡à¸Šà¸™ Kaggle à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸°à¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆà¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹à¸‚à¹ˆà¸‡ Kaggle ğŸ¯

Built with â¤ï¸ for Kaggle beginners and competitions

---

## ğŸ“® Contact & Support

- ğŸ™ **GitHub**: [yourusername/kaggle-utils](https://github.com/yourusername/kaggle-utils)
- ğŸ“§ **Issues**: [Report bugs or request features](https://github.com/yourusername/kaggle-utils/issues)
- ğŸ’¬ **Discussions**: [Ask questions](https://github.com/yourusername/kaggle-utils/discussions)

---

## â­ Star History

If you find this project helpful, please consider giving it a star! â­

---

## ğŸ“ˆ Version History

### v1.0.0 (Current)
- âœ… Initial release
- âœ… 9 modules with comprehensive documentation
- âœ… Support for scikit-learn, LightGBM, XGBoost, CatBoost
- âœ… Interactive visualizations with Plotly
- âœ… Diagnostics module for beginners
- âœ… Complete Thai documentation

---

**Happy Kaggling! ğŸ¯ğŸš€**

## ğŸ“ Project Structure

```
kaggle_utils/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ preprocessing.py
â”œâ”€â”€ models.py
â”œâ”€â”€ ensemble.py
â”œâ”€â”€ outliers.py
â”œâ”€â”€ hyperparams.py
â”œâ”€â”€ visualization.py
â”œâ”€â”€ metrics.py
â”œâ”€â”€ diagnostics.py      ğŸ†• Data validation & model diagnostics
â””â”€â”€ utils.py
```

## ğŸ”§ For Colab Users

```python
# Option 1: Install from GitHub
!pip install git+https://github.com/yourusername/kaggle-utils.git

# Option 2: Clone and install
!git clone https://github.com/yourusername/kaggle-utils.git
%cd kaggle-utils
!pip install -e .

# Import and use
from kaggle_utils import *
setup_colab()
```

## ğŸ“ For ML Beginners

Kaggle Utils à¸¡à¸µ **Diagnostics Module** à¸—à¸µà¹ˆà¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸à¸·à¹ˆà¸­à¸¡à¸·à¸­à¹ƒà¸«à¸¡à¹ˆà¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸°!

**à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸—à¸¸à¸à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸”à¹‰à¸§à¸¢:**
```python
from kaggle_utils.diagnostics import quick_diagnosis

# à¸§à¸´à¸™à¸´à¸ˆà¸‰à¸±à¸¢à¸„à¸£à¸šà¸ˆà¸šà¹ƒà¸™à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹€à¸”à¸µà¸¢à¸§!
report = quick_diagnosis(
    train_df=train,
    target_col='target',
    test_df=test
)
```

**à¸£à¸°à¸šà¸šà¸ˆà¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹ƒà¸«à¹‰à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´:**
- âœ… à¸„à¸¸à¸“à¸ à¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (missing values, constant features)
- âœ… Data Leakage (features à¸—à¸µà¹ˆà¸£à¸±à¹ˆà¸§à¹„à¸«à¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)
- âœ… Multicollinearity (features à¸—à¸µà¹ˆà¸‹à¹‰à¸³à¸‹à¹‰à¸­à¸™)
- âœ… à¹à¸™à¸°à¸™à¸³à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
- âœ… à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸š Overfitting/Underfitting

**à¹à¸¥à¸°à¹ƒà¸«à¹‰à¸„à¸³à¹à¸™à¸°à¸™à¸³à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸§à¹ˆà¸²à¸„à¸§à¸£à¸—à¸³à¸­à¸°à¹„à¸£à¸•à¹ˆà¸­!** ğŸ’¡

ğŸ“– à¸­à¹ˆà¸²à¸™ [Diagnostics Guide](docs/diagnostics_guide.md) à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¸³à¹à¸™à¸°à¸™à¸³à¹‚à¸”à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

Built for the Kaggle community with â¤ï¸

## ğŸ“® Contact

- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com
